{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MNx6ePhSyGAB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Gj6lwdet3s7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d57f6c0-63ea-401e-cc90-f2fb66ac041a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-1095958e857b>:1: DtypeWarning: Columns (544) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_data = pd.read_csv(\"train_data.csv\")\n"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_csv(\"train_data.csv\")\n",
        "X_train = train_data.drop(columns=[\"Activity\"])\n",
        "y_train = train_data[\"Activity\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(\"test_data.csv\")\n",
        "X_test = test_data.drop(columns=[\"Activity\"])\n",
        "y_test = test_data[\"Activity\"]"
      ],
      "metadata": {
        "id": "EfAgoTOCTNEd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9VALfb_L2Fs"
      },
      "source": [
        "<h4>Evaluation Metrics</h4>\n",
        "We compute the following evaluation metrics:\n",
        "\n",
        "* Accuracy: Measures the overall correctness of predictions.\n",
        "* Precision: Evaluates how precise the model's predictions are across all classes.\n",
        "* Recall: Measures the model's ability to identify all relevant instances across all classes.\n",
        "* F1 Score: A balanced measure that combines precision and recall, making it useful for imbalanced datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp28sQibJxDj"
      },
      "source": [
        "<h4>Decision Tree Classifier</h4>\n",
        "Here we evaluate the performance of the Decision Tree Classifier using 5-Fold Cross Validation. Cross-validation is a robust technique for assessing model performance as it splits the dataset into multiple folds, trains on different subsets, and evaluates on the remaining data. This process helps ensure the model's performance generalizes well across unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMTtI83K7OL1",
        "outputId": "0e75e963-2370-409d-9619-43b5e40f8990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier:\n",
            "  Accuracy: 0.5399\n",
            "  Precision: 0.8101\n",
            "  Recall: 0.5336\n",
            "  F1 Score: 0.5740\n"
          ]
        }
      ],
      "source": [
        "dt_model = DecisionTreeClassifier()\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy = cross_val_score(dt_model, X_test, y_test, cv=kf, scoring='accuracy').mean()\n",
        "precision = cross_val_score(dt_model, X_test, y_test, cv=kf, scoring='precision_macro').mean()\n",
        "recall = cross_val_score(dt_model, X_test, y_test, cv=kf, scoring='recall_macro').mean()\n",
        "f1 = cross_val_score(dt_model, X_test, y_test, cv=kf, scoring='f1_macro').mean()\n",
        "\n",
        "print(\"Decision Tree Classifier:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall: {recall:.4f}\")\n",
        "print(f\"  F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHmYdgDzKL6O"
      },
      "source": [
        "<h4>Random Forest Classifier</h4>\n",
        "Here we evaluate the performance of the Random Forest Classifier using 5-Fold Cross Validation. Random Forest is a model that builds multiple decision trees and aggregates their predictions to improve accuracy and reduce overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eUQNe619DM4",
        "outputId": "997c0149-a4bd-49c8-ad2f-9e408f054ada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier:\n",
            "  Accuracy: 0.5680\n",
            "  Precision: 0.8677\n",
            "  Recall: 0.5589\n",
            "  F1 Score: 0.6081\n"
          ]
        }
      ],
      "source": [
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy = cross_val_score(rf_model, X_test, y_test, cv=kf, scoring='accuracy').mean()\n",
        "precision = cross_val_score(rf_model, X_test, y_test, cv=kf, scoring='precision_macro').mean()\n",
        "recall = cross_val_score(rf_model, X_test, y_test, cv=kf, scoring='recall_macro').mean()\n",
        "f1 = cross_val_score(rf_model, X_test, y_test, cv=kf, scoring='f1_macro').mean()\n",
        "\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall: {recall:.4f}\")\n",
        "print(f\"  F1 Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqHfjB55MTaX"
      },
      "source": [
        " <h4>Logistic Regression</h4>\n",
        "\n",
        " Here we evaluate the performance of the Random Forest Classifier using 5-Fold Cross Validation. Random Forest is an ensemble model that builds multiple decision trees and aggregates their predictions to improve accuracy and reduce overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PYmdsj___cKe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "outputId": "9b6641da-1856-400c-9632-b53990c25a9b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1223, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1301, in check_X_y\n    X = check_array(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e2e4bc32c277>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precision_macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'recall_macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;31m# For callable scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             )\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1223, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1301, in check_X_y\n    X = check_array(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
          ]
        }
      ],
      "source": [
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy = cross_val_score(lr_model, X_test, y_test, cv=kf, scoring='accuracy').mean()\n",
        "precision = cross_val_score(lr_model, X_test, y_test, cv=kf, scoring='precision_macro').mean()\n",
        "recall = cross_val_score(lr_model, X_test, y_test, cv=kf, scoring='recall_macro').mean()\n",
        "f1 = cross_val_score(lr_model, X_test, y_test, cv=kf, scoring='f1_macro').mean()\n",
        "\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall: {recall:.4f}\")\n",
        "print(f\"  F1 Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Leave-One-Subject-Out Cross-Validation (LOSO-CV)</h3>\n"
      ],
      "metadata": {
        "id": "NMBCzPfAYmxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "lr_accuracy = cross_val_score(lr_model, X_test, y_test, cv=loo, scoring='accuracy').mean()\n",
        "lr_precision = cross_val_score(lr_model, X_test, y_test, cv=loo, scoring='precision_macro').mean()\n",
        "lr_recall = cross_val_score(lr_model, X_test, y_test, cv=loo, scoring='recall_macro').mean()\n",
        "lr_f1 = cross_val_score(lr_model, X_test, y_test, cv=loo, scoring='f1_macro').mean()\n",
        "\n",
        "rf_accuracy = cross_val_score(rf_model, X_test, y_test, cv=loo, scoring='accuracy').mean()\n",
        "rf_precision = cross_val_score(rf_model, X_test, y_test, cv=loo, scoring='precision_macro').mean()\n",
        "rf_recall = cross_val_score(rf_model, X_test, y_test, cv=loo, scoring='recall_macro').mean()\n",
        "rf_f1 = cross_val_score(rf_model, X_test, y_test, cv=loo, scoring='f1_macro').mean()\n",
        "\n",
        "dt_accuracy = cross_val_score(dt_model, X_test, y_test, cv=loo, scoring='accuracy').mean()\n",
        "dt_precision = cross_val_score(dt_model, X_test, y_test, cv=loo, scoring='precision_macro').mean()\n",
        "dt_recall = cross_val_score(dt_model, X_test, y_test, cv=loo, scoring='recall_macro').mean()\n",
        "dt_f1 = cross_val_score(dt_model, X_test, y_test, cv=loo, scoring='f1_macro').mean()\n",
        "\n",
        "print(\"Logistic Regression (Leave-One-Out CV):\")\n",
        "print(f\"  Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"  Precision: {lr_precision:.4f}\")\n",
        "print(f\"  Recall: {lr_recall:.4f}\")\n",
        "print(f\"  F1 Score: {lr_f1:.4f}\")\n",
        "\n",
        "print(\"Random Forest Classifier (Leave-One-Out CV):\")\n",
        "print(f\"  Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"  Precision: {rf_precision:.4f}\")\n",
        "print(f\"  Recall: {rf_recall:.4f}\")\n",
        "print(f\"  F1 Score: {rf_f1:.4f}\")\n",
        "\n",
        "print(\"Decision Tree Classifier (Leave-One-Out CV):\")\n",
        "print(f\"  Accuracy: {dt_accuracy:.4f}\")\n",
        "print(f\"  Precision: {dt_precision:.4f}\")\n",
        "print(f\"  Recall: {dt_recall:.4f}\")\n",
        "print(f\"  F1 Score: {dt_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "F_gk6x7QWrv9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}